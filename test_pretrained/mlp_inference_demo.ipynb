{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Demo ClipCap MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertguo/miniforge3/envs/clip_prefix_caption/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from typing import Tuple, List, Union, Optional\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel\n",
    ")\n",
    "from enum import Enum\n",
    "\n",
    "class MappingType(Enum):\n",
    "    MLP = 'mlp'\n",
    "    Transformer = 'transformer'\n",
    "\n",
    "class PromptType(Enum):\n",
    "    Empty = \"empty\"\n",
    "    Orginal = 'original'\n",
    "    OriginalWithWords = 'originalplus'\n",
    "N = type(None)\n",
    "V = np.array\n",
    "ARRAY = np.ndarray\n",
    "ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
    "VS = Union[Tuple[V, ...], List[V]]\n",
    "VN = Union[V, N]\n",
    "VNS = Union[VS, N]\n",
    "T = torch.Tensor\n",
    "TS = Union[Tuple[T, ...], List[T]]\n",
    "TN = Optional[T]\n",
    "TNS = Union[Tuple[TN, ...], List[TN]]\n",
    "TSN = Optional[TS]\n",
    "TA = Union[T, ARRAY]\n",
    "D = torch.device\n",
    "CPU = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def forward(self, x: T) -> T:\n",
    "        return self.model(x)\n",
    "\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class CaptionModel(nn.Module):\n",
    "    def get_dummy_token(self, batch_size: int, device: D) -> T:\n",
    "        return torch.zeros(\n",
    "            batch_size, self.prefix_length, dtype=torch.int64, device=device\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None\n",
    "    ):\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(\n",
    "            -1, self.prefix_length, self.gpt_embedding_size\n",
    "        )\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
    "        super(CaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        if prefix_length > 10:  # not enough memory\n",
    "            self.clip_project = nn.Linear(\n",
    "                prefix_size, self.gpt_embedding_size * prefix_length\n",
    "            )\n",
    "        else:\n",
    "            self.clip_project = MLP(\n",
    "                (\n",
    "                    prefix_size,\n",
    "                    (self.gpt_embedding_size * prefix_length) // 2,\n",
    "                    self.gpt_embedding_size * prefix_length,\n",
    "                )\n",
    "            )\n",
    "\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    tokens=None,\n",
    "    prompt=None,\n",
    "    embed=None,\n",
    "    entry_count=1,\n",
    "    entry_length=67,  # maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.0,\n",
    "    stop_token: str = \".\",\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    filter_value = -float(\"Inf\")\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in range(entry_count):\n",
    "            if embed is not None:\n",
    "                generated = embed\n",
    "            else:\n",
    "                if tokens is None:\n",
    "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                    tokens = tokens.unsqueeze(0).to(device)\n",
    "\n",
    "                generated = model.gpt.transformer.wte(tokens)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "\n",
    "                outputs = model.gpt(inputs_embeds=generated)\n",
    "                logits = outputs.logits\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(\n",
    "                    nn.functional.softmax(sorted_logits, dim=-1), dim=-1\n",
    "                )\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
    "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
    "                if tokens is None:\n",
    "                    tokens = next_token\n",
    "                else:\n",
    "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
    "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "                if stop_token_index == next_token.item():\n",
    "                    break\n",
    "\n",
    "            output_list = list(tokens.squeeze().cpu().numpy())\n",
    "            output_text = tokenizer.decode(output_list)\n",
    "            generated_list.append(output_text)\n",
    "\n",
    "    return generated_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptedCaptionModel(nn.Module):\n",
    "\n",
    "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def pad_tokens(self, tokens):\n",
    "        padding = self.max_seq_len - tokens.shape[0]\n",
    "        if padding > 0:\n",
    "            tokens = torch.cat((tokens, torch.zeros(padding, dtype=torch.int64).to(self.device) - 1))\n",
    "        elif padding < 0:\n",
    "            tokens = tokens[:self.max_seq_len]\n",
    "        mask = tokens.ge(0)  # mask is zero where we out of sequence\n",
    "        tokens[~mask] = 0\n",
    "        mask = mask.float()\n",
    "        mask = torch.cat((torch.ones(self.prefix_length).to(self.device), mask), dim=0)  # adding prefix mask\n",
    "        return tokens, mask\n",
    "\n",
    "    \n",
    "    def forward(self, caption, prefix: torch.Tensor,\n",
    "                labels: Optional[torch.Tensor] = None):\n",
    "        # embedding_text = torch.cat((self.prepend_embedding.unsqueeze(0).repeat(40, 1, 1),self.gpt.transformer.wte(tokens)), dim=1 )\n",
    "        # ones_tensor = torch.ones(40, 9).to(device)\n",
    "        # mask = torch.cat((ones_tensor, mask), dim=1)\n",
    "            \n",
    "        # prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "\n",
    "        curr_text = generate(self.original_model, self.tokenizer, embed=prefix_projections)\n",
    "        curr_text = f\" is a picture of {curr_text} and a social media post would caption it {caption}\"\n",
    "        tokens = torch.tensor(self.tokenizer.encode(curr_text)).to(self.device)\n",
    "        tokens, mask = self.pad_tokens(tokens)\n",
    "        embedding_text = self.gpt.transformer.wte(tokens).unsqueeze(0)\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n",
    "                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP, \n",
    "                 prompt_mode: PromptType = PromptType.OriginalWithWords, weights_path: str = \"coco_weights.pt\",\n",
    "                device = \"cpu\" ):\n",
    "        super(PromptedCaptionModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.max_seq_len = 77\n",
    "        self.prompt_mode = prompt_mode\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        if prefix_length > 10:  # not enough memory\n",
    "            self.clip_project = nn.Linear(\n",
    "                prefix_size, self.gpt_embedding_size * prefix_length\n",
    "            )\n",
    "        else:\n",
    "            self.clip_project = MLP(\n",
    "                (\n",
    "                    prefix_size,\n",
    "                    (self.gpt_embedding_size * prefix_length) // 2,\n",
    "                    self.gpt_embedding_size * prefix_length,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if self.prompt_mode == PromptType.OriginalWithWords:\n",
    "            self.original_model = CaptionModel(prefix_length)\n",
    "            state_dict = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "            self.original_model.load_state_dict(state_dict, strict=False)\n",
    "            # Freeze the model parameters\n",
    "            for param in self.original_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "            # prepend_phrase = \"is a image of \"\n",
    "            # prepend_tokens = torch.tensor(self.tokenizer.encode(prepend_phrase)).to(device)\n",
    "            # self.prepend_embedding = self.gpt.transformer.wte(prepend_tokens).detach()\n",
    "            # self.prepend_embedding = self.prepend_embedding.to(device)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Get pretrained weights into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "prefix_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = CaptionModel(prefix_length)\n",
    "model.load_state_dict(torch.load(\"coco_weights.pt\", map_location=CPU), strict=False)\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_finetuned = CaptionModel(prefix_length)\n",
    "model_finetuned.load_state_dict(torch.load(\"base_weights.pt\", map_location=CPU), strict=False)\n",
    "model_finetuned = model_finetuned.eval()\n",
    "model_finetuned = model_finetuned.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'state_dicts/coco_weights.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/albertguo/cs182proj/test_pretrained/mlp_inference_demo.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/albertguo/cs182proj/test_pretrained/mlp_inference_demo.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_finetuned_prompted \u001b[39m=\u001b[39m PromptedCaptionModel(prefix_length)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/albertguo/cs182proj/test_pretrained/mlp_inference_demo.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model_finetuned_prompted\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mprompted_weights_final.pt\u001b[39m\u001b[39m\"\u001b[39m, map_location\u001b[39m=\u001b[39mCPU), strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/albertguo/cs182proj/test_pretrained/mlp_inference_demo.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model_finetuned_prompted \u001b[39m=\u001b[39m model_finetuned_prompted\u001b[39m.\u001b[39meval()\n",
      "\u001b[1;32m/Users/albertguo/cs182proj/test_pretrained/mlp_inference_demo.ipynb Cell 11\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/albertguo/cs182proj/test_pretrained/mlp_inference_demo.ipynb#X33sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_mode \u001b[39m==\u001b[39m PromptType\u001b[39m.\u001b[39mOriginalWithWords:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/albertguo/cs182proj/test_pretrained/mlp_inference_demo.ipynb#X33sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_model \u001b[39m=\u001b[39m CaptionModel(prefix_length)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/albertguo/cs182proj/test_pretrained/mlp_inference_demo.ipynb#X33sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     state_dict \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(weights_path, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/albertguo/cs182proj/test_pretrained/mlp_inference_demo.ipynb#X33sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_model\u001b[39m.\u001b[39mload_state_dict(state_dict, strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/albertguo/cs182proj/test_pretrained/mlp_inference_demo.ipynb#X33sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39m# Freeze the model parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/clip_prefix_caption/lib/python3.8/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniforge3/envs/clip_prefix_caption/lib/python3.8/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    436\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniforge3/envs/clip_prefix_caption/lib/python3.8/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'state_dicts/coco_weights.pt'"
     ]
    }
   ],
   "source": [
    "model_finetuned_prompted = PromptedCaptionModel(prefix_length)\n",
    "model_finetuned_prompted.load_state_dict(torch.load(\"prompted_weights_final.pt\", map_location=CPU), strict=False)\n",
    "model_finetuned_prompted = model_finetuned_prompted.eval()\n",
    "model_finetuned_prompted = model_finetuned_prompted.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Use test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pil_image = Image.open(\"person.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(pil_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image = preprocess(pil_image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
    "prefix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prefix_embedding = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "prefix_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generated_text = generate(model, tokenizer, embed=prefix_embedding)\n",
    "generated_text_finetuned = generate(model_finetuned, tokenizer, embed=prefix_embedding)\n",
    "generated_text_finetuned_prompt = generate(model_finetuned_prompted, tokenizer, embed=prefix_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(pil_image)\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.xlabel(generated_text)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pil_image)\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.xlabel(generated_text_finetuned)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pil_image)\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.xlabel(generated_text_finetuned_prompt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pil_image)\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.xlabel(\"rows and rows\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Use random image from my camera roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pil_image = Image.open(\"dawg.jpg\")\n",
    "image = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
    "prefix_embedding = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "generated_text = generate2(model, tokenizer, embed=prefix_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(pil_image)\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.xlabel(generated_text)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
